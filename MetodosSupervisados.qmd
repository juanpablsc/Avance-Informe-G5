---
title: "Untitled"
format: html
editor: visual
---

## Librerias

```{r}
library(tidyverse)
library(caret)
library(ggplot2)
library(e1071)
library(DataExplorer)
library(pROC)
library(caTools)
library(MASS)
library(dplyr)
library(PRROC)
library(tinytex)
library(corrplot)
library(vioplot)
library(readr)
```

### lectura de datos

```{r}
df_spam <- read.csv('spambase.data')
attach(df_spam)
```

## analisis exploratorio de datos

```{r}
summary(df_spam)
plot_intro(df_spam)
plot_histogram(df_spam)
plot_bar(df_spam)
```

### correlaciones

```{r}
dependiente <- df_spam[,58]
independientes <- names(df_spam)[1:57]
correlaciones_abs <- numeric(length(independientes))

for (i in 1:length(independientes))
  {
  correlation <- cor(dependiente, df_spam[[independientes[i]]])
  
  correlaciones_abs[i] <- abs(correlation)
}

orden <- order(correlaciones_abs)

for (i in orden) {
  cat("Correlaci칩n (valor absoluto) entre", independientes[i], "y la variable dependiente:", correlaciones_abs[i], "\n")
}
```

### Normalizacion de datos

```{r}
# Identificar las columnas a normalizar
columnas_norm <- setdiff(colnames(df_spam), "X1")

# Realizar la normalizaci칩n columna por columna
datos_norm <- as.data.frame(scale(df_spam[, columnas_norm]))

# Combinar la columna no normalizada con las columnas normalizadas
spam_norm <- cbind(df_spam["X1"], datos_norm)
```

```{r}
summary(spam_norm)
plot_intro(spam_norm)
plot_histogram(spam_norm)
plot_bar(spam_norm)
```

### PCA

```{r}
#pca <- prcomp(datos_norm)
#pca

#prop_varianza <- pca$sdev^2 / sum(pca$sdev^2)

#pca_var<-ggplot(data = data.frame(prop_varianza, pc = 1:length(prop_varianza)), aes(x = pc, y = prop_varianza)) + 
  #geom_col(width = 0.3) +  scale_y_continuous(limits = c(0,1)) +  theme_bw() +
  #labs(x = "Componente principal", y = "Prop. de varianza explicada")
#pca_var

#prop_varianza_acum <- cumsum(prop_varianza)

#pca_var_acum<-ggplot(data = data.frame(prop_varianza_acum, pc = 1:length(prop_varianza)), aes(x = pc, y = prop_varianza_acum, group = 1)) +
  #geom_point() +  geom_line() +  theme_bw() +  labs(x = "Componente principal", y = "Prop. varianza explicada acumulada")

#pca_var_acum

#biplot(pca)


#library(factoextra)

#fviz_eig(pca)

#fviz_pca_biplot(pca, repel = TRUE,
                #col.var = "#D2691E", # color para las variables
               # col.ind = "#000000"  # colores de las estaciones
   )               
  # Representaci칩n gr치fica de la importancia de cada variable

#library(corrplot)
#var <- get_pca_var(pca)
#corrplot(var$cos2, is.corr = FALSE)


#plot_prcomp(spam_norm)
```

```{r}
set.seed(163)

for (i in c("X1"))
{
spam_norm[[i]]<-as.factor(spam_norm[[i]])
}
```

```{r}
set.seed(163)

for (i in c("X1"))
{
df_spam[[i]]<-as.factor(df_spam[[i]])}
```

## Evaluacion de metodologias

### division de data train y test

```{r}
set.seed(163)
split<-sample.split(spam_norm$X1,SplitRatio = 0.8)
train<-spam_norm[split==TRUE,]
train_x1<-spam_norm[split==TRUE,'X1']

test<- spam_norm[split==FALSE,]
test_x1<-spam_norm[split==FALSE,'X1']
```

### REGRESION LOGISTICA

#### variables significativas para el modelo

```{r}
fit1 <- glm(formula=X1 ~  X0 + X0.64 + X0.64.1 + X0.1 + X0.32 + X0.2 + X0.3 + X0.4 + X0.5 + X0.6 + X0.7 + X0.64.2 + X0.8 + X0.9 + X0.10 + X0.32.1 + X0.11 + X1.29 + X1.93  + X0.12 + X0.96 + X0.13 + X0.14 + X0.15 +  X0.16 + X0.17 + X0.18 + X0.19 + X0.20 + X0.21 + X0.22 + X0.23 + X0.24 + X0.25 + X0.26 + X0.27 + X0.28 + X0.29 + X0.30 + X0.31  + X0.33 + X0.34 + X0.35 + X0.36 + X0.37 + X0.38 + X0.39 +  X0.40 + X0.41 + X0.42 + X0.43 + X0.778 + X0.44 + X0.45 + X3.756 + X61 + X278,
data=train,family = binomial)
```

```{r}
variables_significativas <- summary(fit1)$coefficients[summary(fit1)$coefficients[, "Pr(>|z|)"] < 0.05, , drop = FALSE]
variables_significativas
```

```{r}
fit2 <- glm(formula=X1 ~  X0.32 + X0.2 + X0.3 + X0.4  + X0.32.1 + X0.11 + X1.93 + X0.96 + X0.14 + X0.15 +  X0.16 + X0.18 + X0.27  + X0.34 + X0.36 + X0.37 + X0.38 +  X0.40 + X0.778 + X0.44 + X61 + X278,
data=train,family = binomial)
summary(fit2)
```

```{r}
set.seed(163)
pred_logistic<-predict(fit2,test,type="response")
y_pred = rep(0, length(pred_logistic))
y_pred[pred_logistic > 0.5] = "1"
```

#### matriz de confusion

```{r}
y_pred<-as.factor(y_pred)
confusionMatrix(y_pred, test$X1, positive = "1")
```

#### ROC y AUC

```{r}
roc_obj <- roc(test$X1,pred_logistic)
plot(roc_obj)
auc(roc_obj)
```

```{r}
coords(roc_obj, "best", ret = c("specificity", "sensitivity", "threshold"))
```

```{r}
y_pred[pred_logistic > 0.3142876	] = "1"
y_pred<-as.factor(y_pred)
confusionMatrix(y_pred, test$X1, positive = "1")
```

### LDA

```{r}
mod_lda <- lda(X1 ~  X0 + X0.64 + X0.64.1 + X0.1 + X0.32 + X0.2 + X0.3 + X0.4 + X0.5 + X0.6 + X0.7 + X0.64.2 + X0.8 + X0.9 + X0.10 + X0.32.1 + X0.11 + X1.29 + X1.93  + X0.12 + X0.96 + X0.13 + X0.14 + X0.15 +  X0.16 + X0.17 + X0.18 + X0.19 + X0.20 + X0.21 + X0.22 + X0.23 + X0.24 + X0.25 + X0.26 + X0.27 + X0.28 + X0.29 + X0.30 + X0.31  + X0.33 + X0.34 + X0.35 + X0.36 + X0.37 + X0.38 + X0.39 +  X0.40 + X0.41 + X0.42 + X0.43 + X0.778 + X0.44 + X0.45 + X3.756 + X61 + X278,data = train)
mod_lda
```

#### matriz de confusion

```{r}
predicciones_lda <- predict(mod_lda, newdata= test)
confusionMatrix(predicciones_lda$class,test$X1,positive = "1")
```

#### ROC y AUC

```{r}
roc_obj_lda <- roc(test$X1,predicciones_lda$posterior[,2])
plot(roc_obj_lda)
auc(roc_obj_lda)
```

### QDA

```{r}
mod_qda <- qda(X1 ~  X0 + X0.64 + X0.64.1 + X0.1 + X0.32 + X0.2 + X0.3 + X0.4 + X0.5 + X0.6 + X0.7 + X0.64.2 + X0.8 + X0.9 + X0.10 + X0.32.1 + X0.11 + X1.29 + X1.93  + X0.12 + X0.96 + X0.13 + X0.14 + X0.15 +  X0.16 + X0.17 + X0.18 + X0.19 + X0.20 + X0.21 + X0.22 + X0.23 + X0.24 + X0.25 + X0.26 + X0.27 + X0.28 + X0.29 + X0.30 + X0.31  + X0.33 + X0.34 + X0.35 + X0.36 + X0.37 + X0.38 + X0.39 +  X0.40 + X0.41 + X0.42 + X0.43 + X0.778 + X0.44 + X0.45 + X3.756 + X61 + X278,data = train)
mod_qda
```

#### matriz de confusion

```{r}
predicciones_qda <- predict(mod_qda, newdata= test)
confusionMatrix(predicciones_qda$class,test$X1,positive = "1")
```

```{r}
roc_obj_qda <- roc(test$X1,predicciones_qda$posterior[,2])
plot(roc_obj_qda)
auc(roc_obj_qda)
```

### KNN

#### modelo con accuracy

```{r}
set.seed(163)
train_knn = train %>% dplyr::select(-c("X1"))
test_knn = test %>% dplyr::select(-c("X1"))
library(class)

overall.accuracy = c()
for (i in 1:20){
  set.seed(163)
knn_pred=knn(train_knn,test_knn,train$X1,k=i)
values = confusionMatrix(table(knn_pred,test$X1))
overall = values$overall
overall.accuracy = append(overall.accuracy , overall["Accuracy"])
}
acc = data.frame(k=1:20, accuracy = overall.accuracy)
```

```{r}
acc = data.frame(k=1:20, accuracy = overall.accuracy)

ggplot(acc) + aes(x = k, y = accuracy) +geom_line(size = 0.5, colour = "#112446") 
```

#### modelo con especificidad

```{r}
overall.specificity <- c()

for (i in 1:20) {
  set.seed(163)
  knn_pred <- knn(train_knn, test_knn, train$X1, k = i)
  values <- confusionMatrix(table(knn_pred, test$X1))
  overall <- values$byClass
  overall.specificity <- append(overall.specificity, overall["Specificity"])
}

acc <- data.frame(k = 1:20, specificity = overall.specificity)

acc = data.frame(k=1:20, specificity = overall.specificity)

ggplot(acc) + aes(x = k, y = specificity) +geom_line(size = 0.5, colour = "#112446") 
```

#### matriz de confusion con k = 3

```{r}
set.seed(163)
knn_pred=knn(train_knn,test_knn,train$X1,k=3,prob=TRUE)
confusionMatrix(knn_pred,test$X1, positive = "1")
```

### Arbol de decision

#### data frame a utilizar

```{r}
set.seed(163)
split<-sample.split(df_spam$X1,SplitRatio = 0.8)
train2<-df_spam[split==TRUE,]
train_x12<-df_spam[split==TRUE,'X1']

test2<- df_spam[split==FALSE,]
test_x12<-df_spam[split==FALSE,'X1']
```

```{r}
library(rpart)
tree_fit = rpart(X1 ~  X0 + X0.64 + X0.64.1 + X0.1 + X0.32 + X0.2 + X0.3 + X0.4 + X0.5 + X0.6 + X0.7 + X0.64.2 + X0.8 + X0.9 + X0.10 + X0.32.1 + X0.11 + X1.29 + X1.93  + X0.12 + X0.96 + X0.13 + X0.14 + X0.15 +  X0.16 + X0.17 + X0.18 + X0.19 + X0.20 + X0.21 + X0.22 + X0.23 + X0.24 + X0.25 + X0.26 + X0.27 + X0.28 + X0.29 + X0.30 + X0.31  + X0.33 + X0.34 + X0.35 + X0.36 + X0.37 + X0.38 + X0.39 +  X0.40 + X0.41 + X0.42 + X0.43 + X0.778 + X0.44 + X0.45 + X3.756 + X61 + X278 , data=train2)
summary(tree_fit)

plot(tree_fit)
text(tree_fit, pretty=3)

tree_plot = rpart.plot(tree_fit)

install.packages("rpart.plot")
library (rpart.plot)
```

```{r}
set.seed(163)
tree_pred = predict(tree_fit, test2 , type ="class")
confusionMatrix(tree_pred,test2$X1, positive = "1")

```

```{r}
tree_pred<-predict(tree_fit,test2,type="prob")
roc_obj4 <- roc(test2$X1,tree_pred[,2])

plot(roc_obj4)
auc(roc_obj4)

```

### SVM

```{r}
glimpse(spam_norm)
```

#### mdata de entrenamiento

```{r}
library(caTools)

set.seed(163) 
split = sample.split(spam_norm$X1, SplitRatio = 0.8)

training_set = subset(spam_norm, split == TRUE) 
test_set = subset(spam_norm, split == FALSE)
```

```{r}
svm_fit <- svm(formula = X1 ~  X0 + X0.64 + X0.64.1 + X0.1 + X0.32 + X0.2 + X0.3 + X0.4 + X0.5 + X0.6 + X0.7 + X0.64.2 + X0.8 + X0.9 + X0.10 + X0.32.1 + X0.11 + X1.29 + X1.93  + X0.12 + X0.96 + X0.13 + X0.14 + X0.15 +  X0.16 + X0.17 + X0.18 + X0.19 + X0.20 + X0.21 + X0.22 + X0.23 + X0.24 + X0.25 + X0.26 + X0.27 + X0.28 + X0.29 + X0.30 + X0.31  + X0.33 + X0.34 + X0.35 + X0.36 + X0.37 + X0.38 + X0.39 +  X0.40 + X0.41 + X0.42 + X0.43 + X0.778 + X0.44 + X0.45 + X3.756 + X61 + X278, data = training_set, kernel = 'linear')

summary(svm_fit)

confusionMatrix(table(true=test_set$X1,pred=predict(svm_fit, newdata=test_set)), positive = "1")

svm_fit2 <- svm(formula = X1 ~ ., data = training_set, kernel = 'radial')

confusionMatrix(table(true=test_set$X1,pred=predict(svm_fit2, newdata=test_set)), positive = "1")

coeficientes <- coef(svm_fit)

intercept <- svm_fit$rho

hiperplano_optimo <- paste("X1 =", paste(coeficientes, collapse = " + "), "+", intercept)

coeficientes1 <- coef(svm_fit2)

intercept1 <- svm_fit2$rho

hiperplano_optimo1 <- paste("X1 =", paste(coeficientes1, collapse = " + "), "+", intercept1)

```

```{r}


# Crear una matriz con las variables explicativas
X <- df_spam[, c("X0", "X0.1", "X0.10", "X0.11", "X0.12", "X0.13", "X0.14", "X0.15", "X0.16", "X0.17", "X0.18", "X0.19", "X0.2", "X0.20", "X0.21", "X0.22", "X0.23","X0.24", "X0.25", "X0.26", "X0.27", "X0.28", "X0.29", "X0.3", "X0.30", "X0.31", "X0.32", "X0.32.1", "X0.33", "X0.34", "X0.35", "X0.36", "X0.37","X0.38", "X0.39", "X0.4", "X0.40", "X0.41", "X0.42", "X0.43", "X0.44", "X0.45", "X0.5", "X0.6", "X0.64", "X0.64.1", "X0.64.2", "X0.7", "X0.778","X0.8", "X0.9", "X0.96", "X1.29", "X1.93", "X278", "X3.756", "X61")]

# Crear un vector con la variable dependiente
y <- df_spam$X1

# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(163)
train_indices <- createDataPartition(y, p = 0.8, list = FALSE)

# Crear conjuntos de entrenamiento y prueba
train_df <- data.frame(X[train_indices, ], y[train_indices])
test_df <- data.frame(X[-train_indices, ], y[-train_indices])

# Crear una lista para almacenar los resultados de accuracy
accuracy_list <- list()

# Iterar por cada combinaci칩n de tres variables
combinations <- combn(names(X), 2, simplify = FALSE)

for (comb in combinations) {
  # Crear un nuevo data frame solo con las variables de la combinaci칩n actual en el conjunto de entrenamiento
  new_df_train <- data.frame(train_df[, comb], y = train_df$y)
  
  # Ajustar el modelo SVM
  svm_fit <- svm(y ~ ., data = new_df_train, kernel = 'radial')
  
  # Realizar predicciones en el conjunto de prueba
  predictions <- predict(svm_fit, newdata = test_df[, comb])
  
  # Calcular accuracy
  accuracy <- sum(predictions == test_df$y) / length(test_df$y)
  
  # Guardar el accuracy en la lista
  accuracy_list[[paste(comb, collapse = "_")]] <- accuracy
}

# Encontrar la combinaci칩n con el mayor accuracy
best_comb <- names(accuracy_list)[which.max(unlist(accuracy_list))]

# Obtener el accuracy correspondiente a la mejor combinaci칩n
best_accuracy <- accuracy_list[[best_comb]]

# Imprimir los resultados
print(paste("Mejor combinaci칩n de variables:", best_comb))
print(paste("Accuracy correspondiente:", best_accuracy))


```

```{r}
# Crear una matriz con las variables explicativas
X <- df_spam[, c("X0", "X0.1", "X0.10", "X0.11", "X0.12", "X0.13", "X0.14", "X0.15", "X0.16", "X0.17", "X0.18", "X0.19", "X0.2", "X0.20", "X0.21", "X0.22", "X0.23","X0.24", "X0.25", "X0.26", "X0.27", "X0.28", "X0.29", "X0.3", "X0.30", "X0.31", "X0.32", "X0.32.1", "X0.33", "X0.34", "X0.35", "X0.36", "X0.37","X0.38", "X0.39", "X0.4", "X0.40", "X0.41", "X0.42", "X0.43", "X0.44", "X0.45", "X0.5", "X0.6", "X0.64", "X0.64.1", "X0.64.2", "X0.7", "X0.778","X0.8", "X0.9", "X0.96", "X1.29", "X1.93", "X278", "X3.756", "X61")]

# Crear un vector con la variable dependiente
y <- df_spam$X1

# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(163)
train_indices <- createDataPartition(y, p = 0.8, list = FALSE)

# Crear conjuntos de entrenamiento y prueba
train_df <- data.frame(X[train_indices, ], y[train_indices])
test_df <- data.frame(X[-train_indices, ], y[-train_indices])

# Crear una lista para almacenar los resultados de accuracy
accuracy_list <- list()

# Iterar por cada combinaci칩n de tres variables
combinations <- combn(names(X), 2, simplify = FALSE)

for (comb in combinations) {
  # Crear un nuevo data frame solo con las variables de la combinaci칩n actual en el conjunto de entrenamiento
  new_df_train <- data.frame(train_df[, comb], y = train_df$y)
  
  # Ajustar el modelo SVM
  svm_fit <- svm(y ~ ., data = new_df_train, kernel = 'radial')
  
  # Realizar predicciones en el conjunto de prueba
  predictions <- predict(svm_fit, newdata = test_df[, comb])
  
  # Calcular accuracy
  accuracy <- sum(predictions == test_df$y) / length(test_df$y)
  
  # Guardar el accuracy en la lista
  accuracy_list[[paste(comb, collapse = "_")]] <- accuracy
}

# Encontrar la combinaci칩n con el mayor accuracy
best_comb <- names(accuracy_list)[which.max(unlist(accuracy_list))]

# Obtener el accuracy correspondiente a la mejor combinaci칩n
best_accuracy <- accuracy_list[[best_comb]]

# Imprimir los resultados
print(paste("Mejor combinaci칩n de variables:", best_comb))
print(paste("Accuracy correspondiente:", best_accuracy))
```

```{r}
# Crear una matriz con las variables explicativas
X <- df_spam[, c("X0", "X0.1", "X0.10", "X0.11", "X0.12", "X0.13", "X0.14", "X0.15", "X0.16", "X0.17", "X0.18", "X0.19", "X0.2", "X0.20", "X0.21", "X0.22", "X0.23","X0.24", "X0.25", "X0.26", "X0.27", "X0.28", "X0.29", "X0.3", "X0.30", "X0.31", "X0.32", "X0.32.1", "X0.33", "X0.34", "X0.35", "X0.36", "X0.37","X0.38", "X0.39", "X0.4", "X0.40", "X0.41", "X0.42", "X0.43", "X0.44", "X0.45", "X0.5", "X0.6", "X0.64", "X0.64.1", "X0.64.2", "X0.7", "X0.778","X0.8", "X0.9", "X0.96", "X1.29", "X1.93", "X278", "X3.756", "X61")]

# Crear un vector con la variable dependiente
y <- df_spam$X1

# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(163)
train_indices <- createDataPartition(y, p = 0.8, list = FALSE)

# Crear conjuntos de entrenamiento y prueba
train_df <- data.frame(X[train_indices, ], y[train_indices])
test_df <- data.frame(X[-train_indices, ], y[-train_indices])

# Crear una lista para almacenar los resultados de accuracy
accuracy_list <- list()

# Iterar por cada combinaci칩n de tres variables
combinations <- combn(names(X), 2, simplify = FALSE)

for (comb in combinations) {
  # Crear un nuevo data frame solo con las variables de la combinaci칩n actual en el conjunto de entrenamiento
  new_df_train <- data.frame(train_df[, comb], y = train_df$y)
  
  # Ajustar el modelo SVM
  svm_fit <- svm(y ~ ., data = new_df_train, kernel = 'linear')
  
  # Realizar predicciones en el conjunto de prueba
  predictions <- predict(svm_fit, newdata = test_df[, comb])
  
  # Calcular accuracy
  accuracy <- sum(predictions == test_df$y) / length(test_df$y)
  
  # Guardar el accuracy en la lista
  accuracy_list[[paste(comb, collapse = "_")]] <- accuracy
}

# Encontrar la combinaci칩n con el mayor accuracy
best_comb <- names(accuracy_list)[which.max(unlist(accuracy_list))]

# Obtener el accuracy correspondiente a la mejor combinaci칩n
best_accuracy <- accuracy_list[[best_comb]]

# Imprimir los resultados
print(paste("Mejor combinaci칩n de variables:", best_comb))
print(paste("Accuracy correspondiente:", best_accuracy))

```

```{r}
svm_fit3 <- svm(formula = X1 ~ X0.44 + X0.778 , data = training_set, kernel = 'linear')
svm_fit4 <- svm(formula = X1 ~ X0.44 + X0.778 , data = training_set, kernel = 'radial')

# Obtener los valores de las variables predictoras
x1 <- training_set$X0.44
x2 <- training_set$X0.778

# Crear una matriz de puntos para el gr치fico
grid_size <- 100
x1_range <- seq(min(x1), max(x1), length.out = grid_size)
x2_range <- seq(min(x2), max(x2), length.out = grid_size)
grid <- expand.grid(X0.44 = x1_range, X0.778 = x2_range)

# Realizar predicciones en la matriz de puntos
predictions <- predict(svm_fit3, newdata = grid)

# Graficar los puntos de datos y las regiones de clasificaci칩n
plot(x1, x2, col = ifelse(training_set$X1 == 1, "blue", "red"), pch = 16, xlab = "X0.44", ylab = "X0.778")
contour(x1_range, x2_range, matrix(predictions, nrow = grid_size), levels = 0, add = TRUE)
legend("topleft", legend = c("Clase 0", "Clase 1"), col = c("red", "blue"), pch = 16)

# Obtener los coeficientes del modelo SVM
coeficientes <- coef(svm_fit3)

# Obtener el intercepto del modelo SVM
intercept <- svm_fit3$rho

# Crear el hiperplano 칩ptimo en formato de cadena de texto
hiperplano_optimo <- paste("X1 =", paste(coeficientes, collapse = " + "), "+", intercept)

# Agregar el hiperplano 칩ptimo al gr치fico
abline(a = -(intercept/coeficientes[2]), b = -(coeficientes[1]/coeficientes[2]), col = "green")

# Imprimir el hiperplano 칩ptimo
text(x = max(x1), y = max(x2), labels = hiperplano_optimo, pos = 4)

```

```{r}
# Ajustar el modelo SVM con kernel radial
svm_fit4 <- svm(formula = X1 ~ X0.44 + X0.778, data = training_set, kernel = 'radial')
plot(svm(formula = X1 ~ X0.44 + X0.778, data = training_set, kernel = 'radial'), training_set)

# Obtener los vectores de soporte
support_vectors <- svm_fit4$SV

# Obtener los coeficientes de peso
weight_coefficients <- svm_fit4$coefs

# Obtener el intercepto
intercept <- svm_fit4$rho

# Imprimir los resultados
print("Vectores de soporte:")
print(support_vectors)
print("Coeficientes de peso:")
print(weight_coefficients)
print("Intercepto:")
print(intercept)



```

```{r}
set.seed(163)
confusionMatrix(table(true=test_set$X1,pred=predict(svm_fit3, newdata=test_set)), positive = "1")
confusionMatrix(table(true=test_set$X1,pred=predict(svm_fit4, newdata=test_set)), positive = "1")
```
